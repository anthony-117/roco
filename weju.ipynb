{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d628e723-c340-4e50-90c9-7722ec6453bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T19:59:13.821510Z",
     "start_time": "2025-11-27T19:59:08.529781Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b67cbfc-4936-43c6-bc00-f5c0c51a604a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T20:00:05.763534Z",
     "start_time": "2025-11-27T19:59:46.310696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    Qwen2_5_VLProcessor\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fb379d0-563d-4dc8-be5c-ec0e0d0323c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T20:00:55.662256Z",
     "start_time": "2025-11-27T20:00:44.989572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PySpark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/28 00:33:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing PySpark...\")\n",
    "\n",
    "# Create Spark session with increased memory and optimizations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LLaVA-FineTuning\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.executor.memory\", \"6g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"200s\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1076317-941f-4764-a3b3-c68dbe36736d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T20:02:23.107132Z",
     "start_time": "2025-11-27T20:02:14.607395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ROCOv2-radiology dataset with PySpark...\n",
      "Dataset info: 79,789 radiological images with captions\n",
      "Source: https://huggingface.co/datasets/eltorio/ROCOv2-radiology\n",
      "\n",
      "Loading training data (59,958 images)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/28 00:33:37 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: ./ROCOv2-radiology/data/train-*.parquet.\n",
      "java.io.FileNotFoundException: File ROCOv2-radiology/data/train-*.parquet does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data (9,904 images)...\n",
      "Loading test data (9,927 images)...\n",
      "\n",
      "======================================================================\n",
      "ROCOv2 DATASET INFORMATION\n",
      "======================================================================\n",
      "\n",
      "Dataset Schema:\n",
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- bytes: binary (nullable = true)\n",
      " |    |-- path: string (nullable = true)\n",
      " |-- image_id: string (nullable = true)\n",
      " |-- caption: string (nullable = true)\n",
      " |-- cui: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "\n",
      "Expected columns: image, image_id, caption, cui (medical concepts)\n",
      "\n",
      "Sample Training Data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/28 00:33:38 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: ./ROCOv2-radiology/data/validation-*.parquet.\n",
      "java.io.FileNotFoundException: File ROCOv2-radiology/data/validation-*.parquet does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/11/28 00:33:38 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: ./ROCOv2-radiology/data/test-*.parquet.\n",
      "java.io.FileNotFoundException: File ROCOv2-radiology/data/test-*.parquet does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|               image|            image_id|             caption|       cui|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|{[89 50 4E 47 0D ...|ROCOv2_2023_train...|Head CT demonstra...|[C0040405]|\n",
      "|{[89 50 4E 47 0D ...|ROCOv2_2023_train...|Acquired renal cy...|[C0041618]|\n",
      "|{[89 50 4E 47 0D ...|ROCOv2_2023_train...|Computed tomograp...|[C0040405]|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Dataset Statistics:\n",
      "Training samples: 59962\n",
      "Validation samples: 9904\n",
      "Test samples: 9927\n",
      "Total samples: 79793\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Loading ROCOv2-radiology dataset with PySpark...\")\n",
    "print(\"Dataset info: 79,789 radiological images with captions\")\n",
    "print(\"Source: https://huggingface.co/datasets/eltorio/ROCOv2-radiology\")\n",
    "\n",
    "# Dataset directory - CHANGE THIS TO YOUR ACTUAL PATH\n",
    "DATASET_DIR = \"./ROCOv2-radiology/data\"  # or \"/path/to/your/data\"\n",
    "\n",
    "# ROCOv2 Dataset Statistics:\n",
    "# - 59,958 training images\n",
    "# - 9,904 validation images  \n",
    "# - 9,927 test images\n",
    "\n",
    "# Load train, validation, and test datasets separately\n",
    "print(\"\\nLoading training data (59,958 images)...\")\n",
    "train_df = spark.read.parquet(f\"{DATASET_DIR}/train-*.parquet\")\n",
    "\n",
    "print(\"Loading validation data (9,904 images)...\")\n",
    "val_df = spark.read.parquet(f\"{DATASET_DIR}/validation-*.parquet\")\n",
    "\n",
    "print(\"Loading test data (9,927 images)...\")\n",
    "test_df = spark.read.parquet(f\"{DATASET_DIR}/test-*.parquet\")\n",
    "\n",
    "# Display schema and sample data\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ROCOv2 DATASET INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nDataset Schema:\")\n",
    "train_df.printSchema()\n",
    "\n",
    "print(\"\\nExpected columns: image, image_id, caption, cui (medical concepts)\")\n",
    "print(\"\\nSample Training Data:\")\n",
    "train_df.show(3, truncate=True)\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "train_count = train_df.count()\n",
    "val_count = val_df.count()\n",
    "test_count = test_df.count()\n",
    "print(f\"Training samples: {train_count}\")\n",
    "print(f\"Validation samples: {val_count}\")\n",
    "print(f\"Test samples: {test_count}\")\n",
    "print(f\"Total samples: {train_count + val_count + test_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e1475d-243a-4cd1-8c50-9e72dd8a5f7e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-11-27T20:02:41.469945Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PREPROCESSING DATA\n",
      "======================================================================\n",
      "\n",
      "Filtering invalid entries...\n",
      "\n",
      "⚠️  SAMPLING 10.0% of data for testing...\n",
      "Set USE_SAMPLE = False to train on full dataset\n",
      "\n",
      "After filtering:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 6111\n",
      "Validation samples: 966\n",
      "\n",
      "Converting to Pandas (this may take a few minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Pandas DataFrames created successfully:\n",
      "Training: 6111 samples\n",
      "Validation: 966 samples\n",
      "\n",
      "Sample training data:\n",
      "                   image_id                                            caption\n",
      "0  ROCOv2_2023_train_000008  Computed tomography of the head on Day 0 shows...\n",
      "1  ROCOv2_2023_train_000017                                  Strawberry skull.\n",
      "\n",
      "Stopping Spark session...\n",
      "\n",
      "✓ Data loading complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPROCESSING DATA\") \n",
    "print(\"=\"*70)\n",
    "\n",
    "# ROCOv2 columns: image, image_id, caption, cui\n",
    "# - image: binary image data\n",
    "# - image_id: unique identifier  \n",
    "# - caption: radiological description\n",
    "# - cui: medical concept codes (array)\n",
    "\n",
    "# Filter out invalid entries\n",
    "print(\"\\nFiltering invalid entries...\")\n",
    "train_df = train_df.filter(\n",
    "    (col(\"image\").isNotNull()) & \n",
    "    (col(\"caption\").isNotNull()) & \n",
    "    (col(\"caption\") != \"\")\n",
    ")\n",
    "\n",
    "val_df = val_df.filter(\n",
    "    (col(\"image\").isNotNull()) & \n",
    "    (col(\"caption\").isNotNull()) &\n",
    "    (col(\"caption\") != \"\")\n",
    ")\n",
    "\n",
    "# OPTIONAL: Sample a subset for faster training and testing\n",
    "# Set USE_SAMPLE = True to test with smaller dataset first\n",
    "USE_SAMPLE = True  # Change to False for full dataset\n",
    "SAMPLE_FRACTION = 0.1  # Use 10% of data for testing\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    print(f\"\\n⚠️  SAMPLING {SAMPLE_FRACTION*100}% of data for testing...\")\n",
    "    train_df = train_df.sample(fraction=SAMPLE_FRACTION, seed=42)\n",
    "    val_df = val_df.sample(fraction=SAMPLE_FRACTION, seed=42)\n",
    "    print(\"Set USE_SAMPLE = False to train on full dataset\")\n",
    "\n",
    "print(f\"\\nAfter filtering:\")\n",
    "train_count = train_df.count()\n",
    "val_count = val_df.count()\n",
    "print(f\"Training samples: {train_count}\")\n",
    "print(f\"Validation samples: {val_count}\")\n",
    "\n",
    "# Convert to Pandas for PyTorch compatibility\n",
    "# Using Arrow for faster conversion\n",
    "print(\"\\nConverting to Pandas (this may take a few minutes)...\")\n",
    "try:\n",
    "    train_data = train_df.toPandas()\n",
    "    eval_data = val_df.toPandas()\n",
    "    \n",
    "    print(f\"\\n✓ Pandas DataFrames created successfully:\")\n",
    "    print(f\"Training: {len(train_data)} samples\")\n",
    "    print(f\"Validation: {len(eval_data)} samples\")\n",
    "    \n",
    "    # Display sample to verify structure\n",
    "    print(\"\\nSample training data:\")\n",
    "    print(train_data[['image_id', 'caption']].head(2))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error during toPandas conversion: {e}\")\n",
    "    print(\"\\nTroubleshooting suggestions:\")\n",
    "    print(\"1. Increase Spark memory in cell 2\")\n",
    "    print(\"2. Enable USE_SAMPLE = True to test with smaller dataset\")\n",
    "    print(\"3. Restart kernel and try again\")\n",
    "    raise\n",
    "\n",
    "# Stop Spark session to free memory\n",
    "print(\"\\nStopping Spark session...\")\n",
    "spark.stop()\n",
    "\n",
    "print(\"\\n✓ Data loading complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8aace3d-04da-4aa7-8f98-8684a7d0d251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINE-TUNING CONFIGURATION FOR CPU\n",
      "======================================================================\n",
      "Model: Qwen/Qwen2.5-VL-3B-Instruct\n",
      "Output directory: ./qwen-lora-finetuned\n",
      "\n",
      "Fine-tuning Qwen2.5-VL on ROCOv2 radiology dataset\n",
      "NOTE: Training on CPU - reduced batch size and epochs for feasibility\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning approach:\n",
    "# 1. Fine-tune Qwen2.5-VL model with LoRA on ROCOv2 radiology dataset\n",
    "# 2. Save the fine-tuned model for inference\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "OUTPUT_DIR = \"./qwen-lora-finetuned\"\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\"\n",
    "}\n",
    "\n",
    "# Training configuration for CPU\n",
    "TRAINING_CONFIG = {\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"num_train_epochs\": 1,  # Reduced for CPU training\n",
    "    \"per_device_train_batch_size\": 1,  # Reduced batch size for CPU\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 16,  # Increased to maintain effective batch size\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"warmup_steps\": 50,  # Reduced for shorter training\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 500,\n",
    "    \"eval_steps\": 500,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"fp16\": False,  # Disabled fp16 for CPU training\n",
    "    \"remove_unused_columns\": False,\n",
    "    \"dataloader_pin_memory\": False,\n",
    "    \"report_to\": \"none\",\n",
    "    \"eval_strategy\": \"steps\"\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINE-TUNING CONFIGURATION FOR CPU\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(\"\\nFine-tuning Qwen2.5-VL on ROCOv2 radiology dataset\")\n",
    "print(\"NOTE: Training on CPU - reduced batch size and epochs for feasibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4a0e00b-3183-415a-8fa3-246e6887e8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen2.5-VL model for CPU training...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Qwen2_5_VLProcessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading Qwen2.5-VL model for CPU training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Processor\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m processor = \u001b[43mQwen2_5_VLProcessor\u001b[49m.from_pretrained(MODEL_NAME)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Model - load without quantization for CPU training\u001b[39;00m\n\u001b[32m      7\u001b[39m model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n\u001b[32m      8\u001b[39m     MODEL_NAME,\n\u001b[32m      9\u001b[39m     torch_dtype=torch.float32,  \u001b[38;5;66;03m# Use float32 for CPU\u001b[39;00m\n\u001b[32m     10\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'Qwen2_5_VLProcessor' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Loading Qwen2.5-VL model for CPU training...\")\n",
    "\n",
    "# Processor\n",
    "processor = Qwen2_5_VLProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Model - load without quantization for CPU training\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,  # Use float32 for CPU\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully for CPU training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0c4bcad-4c31-4e2e-b302-acabdb8337a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model for LoRA training...\n",
      "trainable params: 7,372,800 || all params: 9,311,059,968 || trainable%: 0.0792\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Preparing model for LoRA training...\")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(**LORA_CONFIG)\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fb4c7d3-8538-4ae4-a0f9-e6a7556ccfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating PyTorch datasets...\n",
      "\n",
      "ROCOv2 Dataset initialized with 6111 samples\n",
      "Mode: vqa\n",
      "Columns: ['image', 'image_id', 'caption', 'cui']\n",
      "\n",
      "ROCOv2 Dataset initialized with 966 samples\n",
      "Mode: vqa\n",
      "Columns: ['image', 'image_id', 'caption', 'cui']\n",
      "\n",
      "Dataset created:\n",
      "Training samples: 6111\n",
      "Evaluation samples: 966\n",
      "\n",
      "Note: Fine-tuning LLaVA on medical radiology images (ROCOv2)\n",
      "This model will learn to describe radiological images.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class LLaVADataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset class for ROCOv2-radiology medical imaging dataset\"\"\"\n",
    "    def __init__(self, dataframe, processor, mode='caption'):\n",
    "        self.data = dataframe.to_dict('records')\n",
    "        self.processor = processor\n",
    "        self.mode = mode  # 'caption' or 'vqa'\n",
    "        \n",
    "        print(f\"\\nROCOv2 Dataset initialized with {len(self.data)} samples\")\n",
    "        print(f\"Mode: {self.mode}\")\n",
    "        print(f\"Columns: {list(dataframe.columns)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # ROCOv2 has binary image data stored in 'image' column\n",
    "        image_data = item['image']\n",
    "        \n",
    "        try:\n",
    "            # Handle binary image data from ROCOv2\n",
    "            if isinstance(image_data, bytes):\n",
    "                image = Image.open(BytesIO(image_data)).convert('RGB')\n",
    "            elif isinstance(image_data, dict) and 'bytes' in image_data:\n",
    "                # HuggingFace datasets format\n",
    "                image = Image.open(BytesIO(image_data['bytes'])).convert('RGB')\n",
    "            else:\n",
    "                print(f\"Unexpected image format at index {idx}\")\n",
    "                image = Image.new('RGB', (224, 224), color='black')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image at index {idx}: {e}\")\n",
    "            image = Image.new('RGB', (224, 224), color='black')\n",
    "        \n",
    "        # Get caption (radiological description)\n",
    "        caption = item.get('caption', '')\n",
    "        if caption is None or caption == '':\n",
    "            caption = \"Medical image.\"\n",
    "        \n",
    "        # Format as image captioning task or VQA task\n",
    "        if self.mode == 'vqa':\n",
    "            # Format as visual question answering\n",
    "            questions = [\n",
    "                \"What does this medical image show?\",\n",
    "                \"Describe this radiological image.\",\n",
    "                \"What is visible in this scan?\",\n",
    "                \"What are the findings in this image?\",\n",
    "            ]\n",
    "            question = questions[idx % len(questions)]\n",
    "            answer = caption\n",
    "        else:\n",
    "            # Format as direct captioning\n",
    "            question = \"Describe this medical image.\"\n",
    "            answer = caption\n",
    "        \n",
    "        # LLaVA conversation format\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"<image>\\n{question}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": answer\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Apply chat template\n",
    "        prompt = self.processor.apply_chat_template(\n",
    "            conversation, \n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = self.processor(\n",
    "            text=prompt,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        # Prepare labels\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        \n",
    "        # Mask padding tokens\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        # Find the assistant's response start\n",
    "        # Mask everything before the assistant's response\n",
    "        assistant_start = prompt.find(\"assistant\")\n",
    "        if assistant_start != -1:\n",
    "            tokens_before = self.processor.tokenizer(\n",
    "                prompt[:assistant_start], \n",
    "                add_special_tokens=False\n",
    "            )[\"input_ids\"]\n",
    "            labels[0, :len(tokens_before)] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
    "            \"labels\": labels.squeeze(0)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nCreating PyTorch datasets...\")\n",
    "train_dataset = LLaVADataset(train_data, processor, mode='vqa')\n",
    "eval_dataset = LLaVADataset(eval_data, processor, mode='vqa')\n",
    "\n",
    "print(f\"\\nDataset created:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation samples: {len(eval_dataset)}\")\n",
    "print(\"\\nNote: Fine-tuning LLaVA on medical radiology images (ROCOv2)\")\n",
    "print(\"This model will learn to describe radiological images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ed3ef9-ea64-40d4-a9b6-910403f2a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collator for batching\"\"\"\n",
    "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c62a7fe2-1b92-4974-8d30-2b8574c8de78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up training...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nSetting up training...\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(**TRAINING_CONFIG)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e56c0df8-4832-47b4-a707-ff9f1b9bdb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You can't train a model that has been loaded in 8-bit or 4-bit precision with CPU or disk offload. If you want train the 8-bit or 4-bit model in CPU, please install bitsandbytes with multi-backend, see https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining completed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/roco/.venv/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/roco/.venv/lib/python3.13/site-packages/transformers/trainer.py:2480\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2478\u001b[39m             \u001b[38;5;28mself\u001b[39m.optimizer = \u001b[38;5;28mself\u001b[39m.accelerator.prepare(\u001b[38;5;28mself\u001b[39m.optimizer)\n\u001b[32m   2479\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2480\u001b[39m             model, \u001b[38;5;28mself\u001b[39m.optimizer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2481\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2482\u001b[39m     \u001b[38;5;66;03m# to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\u001b[39;00m\n\u001b[32m   2483\u001b[39m     model, \u001b[38;5;28mself\u001b[39m.optimizer, \u001b[38;5;28mself\u001b[39m.lr_scheduler = \u001b[38;5;28mself\u001b[39m.accelerator.prepare(\n\u001b[32m   2484\u001b[39m         \u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.optimizer, \u001b[38;5;28mself\u001b[39m.lr_scheduler\n\u001b[32m   2485\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/roco/.venv/lib/python3.13/site-packages/accelerate/accelerator.py:1559\u001b[39m, in \u001b[36mAccelerator.prepare\u001b[39m\u001b[34m(self, device_placement, *args)\u001b[39m\n\u001b[32m   1557\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fp8_backend == FP8BackendType.MSAMP:\n\u001b[32m   1558\u001b[39m         args, device_placement = \u001b[38;5;28mself\u001b[39m._prepare_msamp(*args, device_placement=device_placement)\n\u001b[32m-> \u001b[39m\u001b[32m1559\u001b[39m     result = \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1560\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m=\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1562\u001b[39m     result = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m._prepare_one(obj, device_placement=d) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[32m   1563\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer:\n\u001b[32m   1564\u001b[39m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/roco/.venv/lib/python3.13/site-packages/accelerate/accelerator.py:1560\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1557\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fp8_backend == FP8BackendType.MSAMP:\n\u001b[32m   1558\u001b[39m         args, device_placement = \u001b[38;5;28mself\u001b[39m._prepare_msamp(*args, device_placement=device_placement)\n\u001b[32m   1559\u001b[39m     result = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m1560\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m=\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, device_placement)\n\u001b[32m   1561\u001b[39m     )\n\u001b[32m   1562\u001b[39m     result = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m._prepare_one(obj, device_placement=d) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[32m   1563\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer:\n\u001b[32m   1564\u001b[39m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/roco/.venv/lib/python3.13/site-packages/accelerate/accelerator.py:1402\u001b[39m, in \u001b[36mAccelerator._prepare_one\u001b[39m\u001b[34m(self, obj, first_pass, device_placement)\u001b[39m\n\u001b[32m   1400\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prepare_data_loader(obj, device_placement=device_placement)\n\u001b[32m   1401\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch.nn.Module):\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch.optim.Optimizer):\n\u001b[32m   1404\u001b[39m     optimizer = \u001b[38;5;28mself\u001b[39m.prepare_optimizer(obj, device_placement=device_placement)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/roco/.venv/lib/python3.13/site-packages/accelerate/accelerator.py:1825\u001b[39m, in \u001b[36mAccelerator.prepare_model\u001b[39m\u001b[34m(self, model, device_placement, evaluation_mode)\u001b[39m\n\u001b[32m   1816\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1817\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mYou can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt train a model that has been loaded in 8-bit or 4-bit precision on a different device than the one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1818\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33myou\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre training on. Make sure you loaded the model on the correct device using for example `device_map=\u001b[39m\u001b[33m{\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:torch.cuda.current_device()}` or `device_map=\u001b[39m\u001b[33m{\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:torch.xpu.current_device()}`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1819\u001b[39m                 )\n\u001b[32m   1820\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1821\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_devices \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bitsandbytes_multi_backend_available())\n\u001b[32m   1822\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_devices \u001b[38;5;129;01mand\u001b[39;00m is_xpu_available())\n\u001b[32m   1823\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_devices\n\u001b[32m   1824\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1825\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1826\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt train a model that has been loaded in 8-bit or 4-bit precision with CPU or disk offload. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1827\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIf you want train the 8-bit or 4-bit model in CPU, please install bitsandbytes with multi-backend, see https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1828\u001b[39m         )\n\u001b[32m   1829\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m device_placement \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verify_device_map(model):\n\u001b[32m   1830\u001b[39m     model = model.to(\u001b[38;5;28mself\u001b[39m.device)\n",
      "\u001b[31mValueError\u001b[39m: You can't train a model that has been loaded in 8-bit or 4-bit precision with CPU or disk offload. If you want train the 8-bit or 4-bit model in CPU, please install bitsandbytes with multi-backend, see https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting training...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd22f21-d66f-4455-adf4-a6802501abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSaving model to {OUTPUT_DIR}...\")\n",
    "\n",
    "# Save the LoRA adapter\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "processor.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abfb65b-5909-42b3-9260-b6252f05839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING FINAL MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Step 1: Merge LoRA weights with base model\n",
    "print(\"\\n[1/2] Merging LoRA weights with base model...\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Step 2: Save merged model in HuggingFace format\n",
    "MERGED_OUTPUT_DIR = \"./qwen-radiology-merged\"\n",
    "print(f\"[2/2] Saving merged model to {MERGED_OUTPUT_DIR}...\")\n",
    "model.save_pretrained(MERGED_OUTPUT_DIR)\n",
    "processor.save_pretrained(MERGED_OUTPUT_DIR)\n",
    "print(f\"✓ Merged model saved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL EXPORT COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFinal model saved to: {MERGED_OUTPUT_DIR}\")\n",
    "print(f\"You can now use this model for inference or further deployment.\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb0cec6-305f-4bca-ad06-cf5c054a5893",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667c060-e639-4726-973e-d85715c52125",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING THE FINE-TUNED MODEL\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model for testing (CPU version)\n",
    "base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "model.eval()\n",
    "\n",
    "def generate_response(image_data, question):\n",
    "    \"\"\"Generate response for an image and question\"\"\"\n",
    "    try:\n",
    "        # Handle binary image data from ROCOv2 dataset\n",
    "        if isinstance(image_data, bytes):\n",
    "            image = Image.open(BytesIO(image_data)).convert('RGB')\n",
    "        elif isinstance(image_data, dict) and 'bytes' in image_data:\n",
    "            image = Image.open(BytesIO(image_data['bytes'])).convert('RGB')\n",
    "        else:\n",
    "            print(f\"Unexpected image format: {type(image_data)}\")\n",
    "            return \"Error: Could not load image\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return \"Error: Could not load image\"\n",
    "    \n",
    "    # Create conversation in Qwen2.5-VL format\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": question}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = processor.apply_chat_template(\n",
    "        conversation,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Process inputs\n",
    "    inputs = processor(text=prompt, images=[image], return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode response\n",
    "    response = processor.decode(outputs[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=True)\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "# Test with a sample from the dataset\n",
    "if len(eval_data) > 0:\n",
    "    test_sample = eval_data.iloc[0]\n",
    "    print(f\"Testing with sample from evaluation dataset:\")\n",
    "    print(f\"Image ID: {test_sample.get('image_id', 'Unknown')}\")\n",
    "    print(f\"Ground Truth Caption: {test_sample['caption']}\")\n",
    "    print(f\"\\nModel Response:\")\n",
    "    \n",
    "    question = \"Describe this medical image.\"\n",
    "    response = generate_response(test_sample['image'], question)\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"No evaluation data available for testing\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINE-TUNING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"1. LoRA adapter saved at: {OUTPUT_DIR}\")\n",
    "print(f\"2. Use the generate_response() function to test inference\")\n",
    "print(f\"3. Merge weights and save final model when satisfied with results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
